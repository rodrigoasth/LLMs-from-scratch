{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ae38945-39dd-45dc-ad4f-da7a4404241f",
      "metadata": {
        "id": "1ae38945-39dd-45dc-ad4f-da7a4404241f"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181",
      "metadata": {
        "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181"
      },
      "source": [
        "# Capítulo 3: Codificando Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29bcbe8-a034-43a2-b557-997b03c9882d",
      "metadata": {
        "id": "c29bcbe8-a034-43a2-b557-997b03c9882d"
      },
      "source": [
        "Packages that are being used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
      "metadata": {
        "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
        "outputId": "b94bc19e-5559-4ce1-b6fe-7dba9236c2f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.4.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a4474d-7c68-4846-8702-37906cf08197",
      "metadata": {
        "id": "a2a4474d-7c68-4846-8702-37906cf08197"
      },
      "source": [
        "- Este capítulo aborda os mecanismos de atenção, o motor das LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93",
      "metadata": {
        "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/01.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e020fd-9690-4343-80df-da96678bef5e",
      "metadata": {
        "id": "50e020fd-9690-4343-80df-da96678bef5e"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/02.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363",
      "metadata": {
        "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363"
      },
      "source": [
        "## 3.1 O problema ao modelar sequências longas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a",
      "metadata": {
        "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a"
      },
      "source": [
        "- Nenhum código nesta seção.  \n",
        "- Traduzir um texto palavra por palavra não é viável devido às diferenças nas estruturas gramaticais entre a língua de origem e a língua de destino."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c0c433-aa4b-491e-848a-54905ebb05ad",
      "metadata": {
        "id": "55c0c433-aa4b-491e-848a-54905ebb05ad"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/03.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1",
      "metadata": {
        "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1"
      },
      "source": [
        "- Antes da introdução dos modelos Transformer, RNNs encoder-decoder eram comumente usadas para tarefas de tradução automática.  \n",
        "- Nesse modelo, o encoder processa uma sequência de tokens do idioma de origem, utilizando um estado oculto — um tipo de camada intermediária dentro da rede neural — para gerar uma representação condensada de toda a sequência de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d8df2c-c1c2-4df0-9977-ade9713088b2",
      "metadata": {
        "id": "03d8df2c-c1c2-4df0-9977-ade9713088b2"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3602c585-b87a-41c7-a324-c5e8298849df",
      "metadata": {
        "id": "3602c585-b87a-41c7-a324-c5e8298849df"
      },
      "source": [
        "## 3.2 Capturando dependências nos dados com mecanismos de atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fde64c-6034-421d-81d9-8244932086ea",
      "metadata": {
        "id": "b6fde64c-6034-421d-81d9-8244932086ea"
      },
      "source": [
        "- Nenhum código nesta seção.  \n",
        "- Por meio de um mecanismo de atenção, a parte decodificadora da rede, responsável pela geração de texto, é capaz de acessar seletivamente todos os tokens de entrada, o que implica que certos tokens de entrada possuem mais importância do que outros na geração de um token de saída específico."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4f6293-8ab5-4aeb-a04c-50ee158485b1",
      "metadata": {
        "id": "bc4f6293-8ab5-4aeb-a04c-50ee158485b1"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8044be1f-e6a2-4a1f-a6dd-e325d3bad05e",
      "metadata": {
        "id": "8044be1f-e6a2-4a1f-a6dd-e325d3bad05e"
      },
      "source": [
        "- A self-attention em transformers é uma técnica projetada para aprimorar as representações de entrada, permitindo que cada posição em uma sequência interaja com e determine a relevância de todas as outras posições dentro da mesma sequência."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6565dc9f-b1be-4c78-b503-42ccc743296c",
      "metadata": {
        "id": "6565dc9f-b1be-4c78-b503-42ccc743296c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/06.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3",
      "metadata": {
        "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3"
      },
      "source": [
        "## 3.3 Atenção a diferentes partes da entrada com autoatenção"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9af516-7c37-4400-ab53-34936d5495a9",
      "metadata": {
        "id": "6d9af516-7c37-4400-ab53-34936d5495a9"
      },
      "source": [
        "### 3.3.1 Um mecanismo simples de autoatenção sem pesos treináveis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d269e9f1-df11-4644-b575-df338cf46cdf",
      "metadata": {
        "id": "d269e9f1-df11-4644-b575-df338cf46cdf"
      },
      "source": [
        "- Esta seção explica uma variante muito simplificada de autoatenção, que não contém pesos treináveis.  \n",
        "- Isso é apenas para fins ilustrativos e NÃO é o mecanismo de atenção usado nos transformers.  \n",
        "- A próxima seção, seção 3.3.2, ampliará esse mecanismo simples para implementar o mecanismo real de autoatenção.  \n",
        "\n",
        "- Suponha que tenhamos uma sequência de entrada \\( x^{(1)} \\) a \\( x^{(T)} \\):  \n",
        "  - A entrada é um texto (por exemplo, uma frase como \"Sua jornada começa com um passo\") que já foi convertida em embeddings de tokens, conforme descrito no capítulo 2.  \n",
        "  - Por exemplo, \\( x^{(1)} \\) é um vetor de dimensão \\( d \\) representando a palavra \"Sua\", e assim por diante.  \n",
        "\n",
        "- **Objetivo:** calcular os vetores de contexto \\( z^{(i)} \\) para cada elemento da sequência de entrada \\( x^{(i)} \\) em \\( x^{(1)} \\) a \\( x^{(T)} \\) (onde \\( z \\) e \\( x \\) têm a mesma dimensão).  \n",
        "  - Um vetor de contexto \\( z^{(i)} \\) é uma soma ponderada das entradas \\( x^{(1)} \\) a \\( x^{(T)} \\).  \n",
        "  - O vetor de contexto é específico para cada entrada.  \n",
        "    - Em vez de considerar \\( x^{(i)} \\) como um marcador genérico para qualquer token de entrada, vamos analisar o segundo token de entrada, \\( x^{(2)} \\).  \n",
        "    - E, para um exemplo mais concreto, em vez de \\( z^{(i)} \\), consideramos o segundo vetor de contexto de saída, \\( z^{(2)} \\).  \n",
        "    - O vetor de contexto \\( z^{(2)} \\) é uma soma ponderada de todas as entradas \\( x^{(1)} \\) a \\( x^{(T)} \\), ponderadas em relação ao segundo elemento de entrada, \\( x^{(2)} \\).  \n",
        "    - Os pesos de atenção determinam quanto cada um dos elementos de entrada contribui para a soma ponderada ao calcular \\( z^{(2)} \\).  \n",
        "    - Resumindo, podemos pensar em \\( z^{(2)} \\) como uma versão modificada de \\( x^{(2)} \\) que também incorpora informações sobre outros elementos da entrada que são relevantes para a tarefa em questão."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a",
      "metadata": {
        "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"400px\">\n",
        "\n",
        "- (Observe que os números nesta figura foram truncados para uma casa decimal após o ponto para reduzir a poluição visual; da mesma forma, outras figuras também podem conter valores truncados)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff856c58-8382-44c7-827f-798040e6e697",
      "metadata": {
        "id": "ff856c58-8382-44c7-827f-798040e6e697"
      },
      "source": [
        "- Por convenção, os pesos de atenção não normalizados são chamados de **\"attention scores\"**, enquanto os scores de atenção normalizados, que somam 1, são chamados de **\"attention weights\"**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b10344-128d-462a-823f-2178dff5fd58",
      "metadata": {
        "id": "01b10344-128d-462a-823f-2178dff5fd58"
      },
      "source": [
        "- O código abaixo percorre a figura acima passo a passo\n",
        "\n",
        "<br>\n",
        "\n",
        "- Passo 1: calcular os scores de atenção não normalizados 𝜔\n",
        "\n",
        "- Suponha que usamos o segundo token de entrada como a consulta (query), ou seja, $q^{(2)} = x^{(2)}$. Calculamos os scores de atenção não normalizados usando produtos escalares:\n",
        "    - $\\omega_{21} = x^{(1)} q^{(2)\\top}$\n",
        "    - $\\omega_{22} = x^{(2)} q^{(2)\\top}$\n",
        "    - $\\omega_{23} = x^{(3)} q^{(2)\\top}$\n",
        "    - ...\n",
        "    - $\\omega_{2T} = x^{(T)} q^{(2)\\top}$\n",
        "- Acima, $\\omega$ é a letra grega \"ômega\", usada para simbolizar os scores de atenção não normalizados.\n",
        "    - O subíndice \"21\" em $\\omega_{21}$ significa que o segundo elemento da sequência de entrada foi usado como consulta (query) contra o primeiro elemento da sequência de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e55f7a-f2d0-4f24-858b-228e4fe88fb3",
      "metadata": {
        "id": "35e55f7a-f2d0-4f24-858b-228e4fe88fb3"
      },
      "source": [
        "- Suponha que temos a seguinte sentença de entrada, que já foi incorporada em vetores tridimensionais, conforme descrito no capítulo 3 (aqui usamos uma dimensão de embedding muito pequena apenas para fins ilustrativos, para que caiba na página sem quebras de linha)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3",
      "metadata": {
        "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83",
      "metadata": {
        "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83"
      },
      "source": [
        "- (Neste livro, seguimos a convenção comum de aprendizado de máquina e aprendizado profundo, onde os exemplos de treinamento são representados como linhas e os valores das características como colunas; no caso do tensor mostrado acima, cada linha representa uma palavra e cada coluna representa uma dimensão do embedding).  \n",
        "\n",
        "- O principal objetivo desta seção é demonstrar como o vetor de contexto $z^{(2)}$  é calculado usando a segunda sequência de entrada, $x^{(2)}$, como consulta (*query*).  \n",
        "\n",
        "- A figura ilustra a etapa inicial desse processo, que envolve o cálculo dos scores de atenção ω entre $x^{(2)}$ e todos os outros elementos de entrada por meio de uma operação de produto escalar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb3453a-58fa-42c4-b225-86850bc856f8",
      "metadata": {
        "id": "5cb3453a-58fa-42c4-b225-86850bc856f8"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/08.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77be52fb-82fd-4886-a4c8-f24a9c87af22",
      "metadata": {
        "id": "77be52fb-82fd-4886-a4c8-f24a9c87af22"
      },
      "source": [
        "- Usamos o elemento 2 da sequência de entrada, $x^{(2)}$, como exemplo para calcular o vetor de contexto $z^{(2)}$; mais adiante nesta seção, iremos generalizar esse processo para calcular todos os vetores de contexto.  \n",
        "- O primeiro passo é calcular os scores de atenção não normalizados realizando o produto escalar entre a consulta $x^{(2)}$ e todos os outros tokens de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
      "metadata": {
        "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
        "outputId": "754b0f2e-d9dc-48a3-babd-fc8726f055b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1]  # O segundo token de entrada é a consulta\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # produto escalar (transposição não é necessária aqui, pois são vetores unidimensionais)\n",
        "\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df09ae0-199f-4b6f-81a0-2f70546684b8",
      "metadata": {
        "id": "8df09ae0-199f-4b6f-81a0-2f70546684b8"
      },
      "source": [
        "- Observação: um produto escalar é essencialmente uma forma simplificada de multiplicar dois vetores elemento por elemento e somar os produtos resultantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
      "metadata": {
        "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
        "outputId": "7030daed-c5ed-4943-af38-713a8cb30991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9544)\n",
            "tensor(0.9544)\n"
          ]
        }
      ],
      "source": [
        "res = 0.\n",
        "\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "    res += inputs[0][idx] * query[idx]\n",
        "\n",
        "print(res)\n",
        "print(torch.dot(inputs[0], query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d444d76-e19e-4e9a-a268-f315d966609b",
      "metadata": {
        "id": "7d444d76-e19e-4e9a-a268-f315d966609b"
      },
      "source": [
        "- Passo 2: normalizar os scores de atenção não normalizados (\"ômega\", $\\omega$) para que sua soma seja igual a 1.  \n",
        "- Aqui está uma maneira simples de normalizar os scores de atenção não normalizados para que somem 1 (uma convenção útil para interpretação e importante para a estabilidade do treinamento)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd965d6-980c-476a-93d8-9efe603b1b3b",
      "metadata": {
        "id": "dfd965d6-980c-476a-93d8-9efe603b1b3b"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
      "metadata": {
        "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
        "outputId": "aa3216d8-9b78-4713-ef55-31d24fd2f514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "print(\"Sum:\", attn_weights_2_tmp.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75dc0a57-f53e-41bf-8793-daa77a819431",
      "metadata": {
        "id": "75dc0a57-f53e-41bf-8793-daa77a819431"
      },
      "source": [
        "- No entanto, na prática, é comum e recomendado usar a função softmax para normalização, pois ela lida melhor com valores extremos e possui propriedades de gradiente mais desejáveis durante o treinamento.  \n",
        "- Aqui está uma implementação simples da função softmax para escalonamento, que também normaliza os elementos do vetor para que sua soma seja igual a 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
      "metadata": {
        "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
        "outputId": "6882f9aa-6eda-4693-9c87-1e48b8c78637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb",
      "metadata": {
        "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb"
      },
      "source": [
        "- A implementação ingênua acima pode sofrer com problemas de instabilidade numérica para valores de entrada muito grandes ou muito pequenos, devido a problemas de overflow e underflow.  \n",
        "- Portanto, na prática, é recomendado usar a implementação do softmax do PyTorch, que foi altamente otimizada para desempenho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
      "metadata": {
        "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
        "outputId": "031b23cd-0fda-4f61-c17e-2837088f222d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f",
      "metadata": {
        "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f"
      },
      "source": [
        "- Passo 3: calcular o vetor de contexto $z^{(2)}$ multiplicando os tokens de entrada incorporados, $x^{(i)}$, pelos pesos de atenção e somando os vetores resultantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c9f5ac-8d3d-4847-94e3-fd783b7d4d3d",
      "metadata": {
        "id": "f1c9f5ac-8d3d-4847-94e3-fd783b7d4d3d"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
      "metadata": {
        "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
        "outputId": "f5c88e77-3632-470d-a5d4-487168538e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1] # 2nd input token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89",
      "metadata": {
        "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89"
      },
      "source": [
        "### 3.3.2 Calculando os pesos de atenção para todos os tokens de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a02bb73-fc19-4c88-b155-8314de5d63a8",
      "metadata": {
        "id": "6a02bb73-fc19-4c88-b155-8314de5d63a8"
      },
      "source": [
        "#### Generalizar para todos os tokens da sequência de entrada:  \n",
        "\n",
        "- Acima, calculamos os pesos de atenção e o vetor de contexto para o segundo token de entrada (como ilustrado na linha destacada na figura abaixo).  \n",
        "- Agora, estamos generalizando esse cálculo para computar todos os pesos de atenção e vetores de contexto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4",
      "metadata": {
        "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/11.webp\" width=\"400px\">\n",
        "\n",
        "- (Please note that the numbers in this figure are truncated to two\n",
        "digits after the decimal point to reduce visual clutter; the values in each row should add up to 1.0 or 100%; similarly, digits in other figures are truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b789b990-fb51-4beb-9212-bf58876b5983",
      "metadata": {
        "id": "b789b990-fb51-4beb-9212-bf58876b5983"
      },
      "source": [
        "- Na autoatenção, o processo começa com o cálculo dos scores de atenção, que são posteriormente normalizados para obter os pesos de atenção, cuja soma totaliza 1.  \n",
        "- Esses pesos de atenção são então utilizados para gerar os vetores de contexto por meio de uma soma ponderada das entradas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c",
      "metadata": {
        "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/12.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa652506-f2c8-473c-a905-85c389c842cc",
      "metadata": {
        "id": "aa652506-f2c8-473c-a905-85c389c842cc"
      },
      "source": [
        "- Aplicar o **passo 1** anterior a todos os pares de elementos para calcular a matriz de scores de atenção não normalizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04004be8-07a1-468b-ab33-32e16a551b45",
      "metadata": {
        "id": "04004be8-07a1-468b-ab33-32e16a551b45",
        "outputId": "02d21fc5-1d43-462f-a2b6-214ca2c05aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4",
      "metadata": {
        "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4"
      },
      "source": [
        "- Podemos obter o mesmo resultado de forma mais eficiente por meio da multiplicação de matrizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
      "metadata": {
        "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
        "outputId": "fdb36c24-ba42-422e-be3a-e2657ca05200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c4bac4-acfd-427f-9b11-c436ac71748d",
      "metadata": {
        "id": "02c4bac4-acfd-427f-9b11-c436ac71748d"
      },
      "source": [
        "- Similar ao **passo 2** anterior, normalizamos cada linha para que os valores em cada linha somem 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
      "metadata": {
        "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
        "outputId": "ed976ec7-691b-45f3-e204-236ce803b0ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c",
      "metadata": {
        "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c"
      },
      "source": [
        "- Verificação rápida para confirmar que os valores em cada linha realmente somam 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
      "metadata": {
        "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
        "outputId": "227b3bbc-b1ce-44ec-c974-16df83478a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1",
      "metadata": {
        "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1"
      },
      "source": [
        "- Aplicar o **passo 3** anterior para calcular todos os vetores de contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
      "metadata": {
        "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
        "outputId": "95634ece-5035-49da-fde4-27c4b38dd4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b245b8-7732-4fab-aa1c-e3d333195605",
      "metadata": {
        "id": "25b245b8-7732-4fab-aa1c-e3d333195605"
      },
      "source": [
        "- Como uma verificação de consistência, o vetor de contexto previamente calculado $z^{(2)} = [0.4419, 0.6515, 0.5683]$ pode ser encontrado na segunda linha acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
      "metadata": {
        "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
        "outputId": "de3e1438-d655-4965-ad0b-ddab6a956100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525",
      "metadata": {
        "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525"
      },
      "source": [
        "## 3.4 Implementando autoatenção com pesos treináveis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88363117-93d8-41fb-8240-f7cfe08b14a3",
      "metadata": {
        "id": "88363117-93d8-41fb-8240-f7cfe08b14a3"
      },
      "source": [
        "- Um modelo conceitual ilustrando como o mecanismo de autoatenção desenvolvido nesta seção se integra à narrativa e estrutura geral deste livro e capítulo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9492ba-6f66-4f65-bd1d-87cf16d59928",
      "metadata": {
        "id": "ac9492ba-6f66-4f65-bd1d-87cf16d59928"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/13.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b90a77e-d746-4704-9354-1ddad86e6298",
      "metadata": {
        "id": "2b90a77e-d746-4704-9354-1ddad86e6298"
      },
      "source": [
        "### 3.4.1 Calculando os pesos de atenção passo a passo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036",
      "metadata": {
        "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036"
      },
      "source": [
        "Nesta seção, estamos implementando o mecanismo de autoatenção utilizado na arquitetura original dos transformers, nos modelos GPT e na maioria das LLMs populares.  \n",
        "Esse mecanismo de autoatenção também é chamado de \"atenção por produto escalar escalado\" (*scaled dot-product attention*).  \n",
        "\n",
        "A ideia geral é semelhante à apresentada anteriormente:  \n",
        "- Queremos calcular vetores de contexto como somas ponderadas dos vetores de entrada, específicos para um determinado elemento da sequência.  \n",
        "- Para isso, precisamos de pesos de atenção.  \n",
        "\n",
        "Como veremos, há apenas pequenas diferenças em relação ao mecanismo de atenção básico introduzido anteriormente:  \n",
        "- A diferença mais notável é a introdução de **matrizes de pesos** que são atualizadas durante o treinamento do modelo.  \n",
        "- Essas **matrizes de pesos treináveis** são fundamentais para que o modelo (especificamente o módulo de atenção dentro do modelo) possa aprender a produzir **vetores de contexto \"bons\"**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59db4093-93e8-4bee-be8f-c8fac8a08cdd",
      "metadata": {
        "id": "59db4093-93e8-4bee-be8f-c8fac8a08cdd"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/14.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d996671-87aa-45c9-b2e0-07a7bcc9060a",
      "metadata": {
        "id": "4d996671-87aa-45c9-b2e0-07a7bcc9060a"
      },
      "source": [
        "- Implementando o mecanismo de autoatenção passo a passo, começaremos introduzindo as três matrizes de pesos treináveis. $W_q$, $W_k$, e $W_v$\n",
        "- Implementando o mecanismo de autoatenção passo a passo, começaremos introduzindo as três matrizes de pesos treináveis. $x^{(i)}$, em query, key, and value vectors via matrix multiplication:\n",
        "\n",
        "  - Query vector: $q^{(i)} = W_q \\,x^{(i)}$\n",
        "  - Key vector: $k^{(i)} = W_k \\,x^{(i)}$\n",
        "  - Value vector: $v^{(i)} = W_v \\,x^{(i)}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f334313-5fd0-477b-8728-04080a427049",
      "metadata": {
        "id": "9f334313-5fd0-477b-8728-04080a427049"
      },
      "source": [
        "- As dimensões de embedding da entrada $x$ e do vetor de consulta $q$ podem ser iguais ou diferentes, dependendo do design e da implementação específica do modelo.  \n",
        "- Nos modelos GPT, as dimensões de entrada e saída geralmente são as mesmas, mas, para fins ilustrativos e para melhor acompanhar os cálculos, escolhemos dimensões de entrada e saída diferentes aqui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db",
      "metadata": {
        "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db"
      },
      "outputs": [],
      "source": [
        "x_2 = inputs[1] # second input element\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2 # the output embedding size, d=2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf",
      "metadata": {
        "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf"
      },
      "source": [
        "- Abaixo, inicializamos as três matrizes de pesos; observe que estamos definindo `requires_grad=False` para reduzir a complexidade das saídas para fins ilustrativos, mas, se fôssemos usar essas matrizes de pesos para o treinamento do modelo, definiríamos `requires_grad=True` para que elas fossem atualizadas durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00",
      "metadata": {
        "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abfd0b50-7701-4adb-821c-e5433622d9c4",
      "metadata": {
        "id": "abfd0b50-7701-4adb-821c-e5433622d9c4"
      },
      "source": [
        "- Em seguida, calculamos os vetores de consulta (*query*), chave (*key*) e valor (*value*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73cedd62-01e1-4196-a575-baecc6095601",
      "metadata": {
        "id": "73cedd62-01e1-4196-a575-baecc6095601",
        "outputId": "19575837-e362-4707-c1f7-2ac8ba0adc5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(query_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be308b3-aca3-421b-b182-19c3a03b71c7",
      "metadata": {
        "id": "9be308b3-aca3-421b-b182-19c3a03b71c7"
      },
      "source": [
        "- Como podemos ver abaixo, projetamos com sucesso os 6 tokens de entrada de um espaço de embedding 3D para um espaço de embedding 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
      "metadata": {
        "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
        "outputId": "00b319ca-9938-4bd0-a7b7-797845bcf5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481",
      "metadata": {
        "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481"
      },
      "source": [
        "- No próximo passo, **passo 2**, calculamos os scores de atenção não normalizados realizando o produto escalar entre a consulta (*query*) e cada vetor de chave (*key*)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa",
      "metadata": {
        "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/15.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cbc253-a182-4490-a765-246979ea0a28",
      "metadata": {
        "id": "64cbc253-a182-4490-a765-246979ea0a28",
        "outputId": "c3bc2b0a-3244-4049-ff99-81474ac6d3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1] # Python starts index at 0\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935",
      "metadata": {
        "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935"
      },
      "source": [
        "- Como temos 6 entradas, obtemos 6 scores de atenção para o vetor de consulta fornecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14e44b5-d170-40f9-8847-8990804af26d",
      "metadata": {
        "id": "b14e44b5-d170-40f9-8847-8990804af26d",
        "outputId": "3792a9e0-ef22-4dbb-aeb0-d4b3e220bef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999",
      "metadata": {
        "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/16.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1609edb-f089-461a-8de2-c20c1bb29836",
      "metadata": {
        "id": "e1609edb-f089-461a-8de2-c20c1bb29836"
      },
      "source": [
        "- Em seguida, no **passo 3**, calculamos os pesos de atenção (scores de atenção normalizados que somam 1) usando a função softmax que utilizamos anteriormente.  \n",
        "- A diferença em relação ao que fizemos antes é que agora escalamos os scores de atenção dividindo-os pela raiz quadrada da dimensão do embedding, $\\sqrt{d_k}$  (ou seja, `d_k**0.5`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146f5587-c845-4e30-9894-c7ed3a248153",
      "metadata": {
        "id": "146f5587-c845-4e30-9894-c7ed3a248153",
        "outputId": "6b677849-d378-4151-a412-5ac4d2eea55a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f61a28-b103-434a-aee1-ae7cbd821126",
      "metadata": {
        "id": "b8f61a28-b103-434a-aee1-ae7cbd821126"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f",
      "metadata": {
        "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f"
      },
      "source": [
        "- No **passo 4**, agora calculamos o vetor de contexto para o vetor de consulta da entrada 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
      "metadata": {
        "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
        "outputId": "9137461e-2f87-4b66-fcbf-101602c209aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7b2907-e448-473e-b46c-77735a7281d8",
      "metadata": {
        "id": "9d7b2907-e448-473e-b46c-77735a7281d8"
      },
      "source": [
        "### 3.4.2 Implementando uma classe compacta de SelfAttention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04313410-3155-4d90-a7a3-2f3386e73677",
      "metadata": {
        "id": "04313410-3155-4d90-a7a3-2f3386e73677"
      },
      "source": [
        "- Reunindo tudo, podemos implementar o mecanismo de autoatenção da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
      "metadata": {
        "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
        "outputId": "31447f6b-9e9e-49d6-e9df-fa3199be9cd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445",
      "metadata": {
        "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081",
      "metadata": {
        "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081"
      },
      "source": [
        "- Podemos simplificar a implementação acima utilizando as camadas **Linear** do PyTorch, que são equivalentes a uma multiplicação de matrizes quando desativamos as unidades de viés.  \n",
        "- Outra grande vantagem de usar `nn.Linear` em vez da abordagem manual com `nn.Parameter(torch.rand(...))` é que `nn.Linear` possui um esquema de inicialização de pesos preferencial, o que resulta em um treinamento do modelo mais estável."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
      "metadata": {
        "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
        "outputId": "a11f2dbd-e13f-4e45-a7af-f5a2f66c6993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce",
      "metadata": {
        "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce"
      },
      "source": [
        "- Observe que `SelfAttention_v1` e `SelfAttention_v2` produzem saídas diferentes porque utilizam pesos iniciais distintos para as matrizes de pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab",
      "metadata": {
        "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab"
      },
      "source": [
        "## 3.5 Ocultando palavras futuras com atenção causal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3",
      "metadata": {
        "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3"
      },
      "source": [
        "- Na atenção causal, os pesos de atenção acima da diagonal são mascarados, garantindo que, para qualquer entrada, a LLM não possa utilizar tokens futuros ao calcular os vetores de contexto com os pesos de atenção."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35",
      "metadata": {
        "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba",
      "metadata": {
        "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba"
      },
      "source": [
        "### 3.5.1 Aplicando uma máscara de atenção causal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218",
      "metadata": {
        "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218"
      },
      "source": [
        "- Nesta seção, estamos convertendo o mecanismo de autoatenção anterior em um mecanismo de autoatenção causal.  \n",
        "- A autoatenção causal garante que a previsão do modelo para uma determinada posição em uma sequência dependa apenas das saídas conhecidas nas posições anteriores, e não em posições futuras.  \n",
        "- Em outras palavras, isso assegura que cada previsão da próxima palavra dependa apenas das palavras anteriores.  \n",
        "- Para alcançar esse objetivo, mascaramos os tokens futuros para cada token dado (ou seja, aqueles que vêm depois do token atual no texto de entrada)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f99af3-32bc-48f5-8eb4-63504670ca0a",
      "metadata": {
        "id": "57f99af3-32bc-48f5-8eb4-63504670ca0a"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/20.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbfaec7a-68f2-4157-a4b5-2aeceed199d9",
      "metadata": {
        "id": "cbfaec7a-68f2-4157-a4b5-2aeceed199d9"
      },
      "source": [
        "- Para ilustrar e implementar a autoatenção causal, vamos trabalhar com os scores de atenção e pesos da seção anterior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
      "metadata": {
        "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
        "outputId": "e3ba61be-5e9f-4501-d93f-6a24903427d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Reutilize as matrizes de pesos de consulta e chave do\n",
        "# objeto SelfAttention_v2 da seção anterior para conveniência\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6",
      "metadata": {
        "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6"
      },
      "source": [
        "- A maneira mais simples de mascarar os pesos de atenção futuros é criando uma máscara usando a função `tril` do PyTorch, com os elementos abaixo da diagonal principal (incluindo a diagonal) definidos como 1 e acima da diagonal principal definidos como 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
      "metadata": {
        "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
        "outputId": "396c4476-be8e-4773-d984-c92d60fec0a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efce2b08-3583-44da-b3fc-cabdd38761f6",
      "metadata": {
        "id": "efce2b08-3583-44da-b3fc-cabdd38761f6"
      },
      "source": [
        "- Em seguida, podemos multiplicar os pesos de atenção por essa máscara para zerar os scores de atenção acima da diagonal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
      "metadata": {
        "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
        "outputId": "94bb06bf-a221-402d-fc2e-98cfb5e304ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb35787-cf12-4024-b66d-e7215e175500",
      "metadata": {
        "id": "3eb35787-cf12-4024-b66d-e7215e175500"
      },
      "source": [
        "- No entanto, se a máscara fosse aplicada após o softmax, como acima, isso perturbaria a distribuição de probabilidades criada pelo softmax.  \n",
        "- O softmax garante que todos os valores de saída somem 1.  \n",
        "- Aplicar a máscara após o softmax exigiria re-normalizar as saídas para que somassem 1 novamente, o que complicaria o processo e poderia levar a efeitos indesejados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f",
      "metadata": {
        "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f"
      },
      "source": [
        "- Para garantir que as linhas somem 1, podemos normalizar os pesos de atenção da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
      "metadata": {
        "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
        "outputId": "87d87077-1aca-4523-fe3d-e9e997e4a3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877",
      "metadata": {
        "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877"
      },
      "source": [
        "- Embora tecnicamente já tenhamos finalizado o código do mecanismo de atenção causal, vamos dar uma olhada brevemente em uma abordagem mais eficiente para alcançar o mesmo resultado que o acima.  \n",
        "- Assim, em vez de zerar os pesos de atenção acima da diagonal e renormalizar os resultados, podemos mascarar os scores de atenção não normalizados acima da diagonal com **menos infinito** antes de entrarem na função softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb682900-8df2-4767-946c-a82bee260188",
      "metadata": {
        "id": "eb682900-8df2-4767-946c-a82bee260188"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/21.webp\" width=\"450px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
      "metadata": {
        "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
        "outputId": "9b2fc28d-1f49-47ee-d335-1ccc1b695a51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d5f803-d735-4543-b9da-00ac10fb9c50",
      "metadata": {
        "id": "91d5f803-d735-4543-b9da-00ac10fb9c50"
      },
      "source": [
        "- Como podemos ver abaixo, agora os pesos de atenção em cada linha somam corretamente 1 novamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
      "metadata": {
        "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
        "outputId": "4f6ac9a3-b6bb-4aa0-89ab-5f52b35ae41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912",
      "metadata": {
        "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912"
      },
      "source": [
        "### 3.5.2 Mascarando pesos de atenção adicionais com dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a",
      "metadata": {
        "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a"
      },
      "source": [
        "- Além disso, também aplicamos dropout para reduzir o overfitting durante o treinamento.  \n",
        "- O dropout pode ser aplicado em vários lugares:  \n",
        "  - por exemplo, após calcular os pesos de atenção;  \n",
        "  - ou após multiplicar os pesos de atenção pelos vetores de valor.  \n",
        "- Aqui, aplicaremos a máscara de dropout após calcular os pesos de atenção, pois isso é mais comum.  \n",
        "\n",
        "- Além disso, neste exemplo específico, usamos uma taxa de dropout de 50%, o que significa mascarar aleatoriamente metade dos pesos de atenção. (Quando treinarmos o modelo GPT mais tarde, usaremos uma taxa de dropout mais baixa, como 0,1 ou 0,2)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee799cf6-6175-45f2-827e-c174afedb722",
      "metadata": {
        "id": "ee799cf6-6175-45f2-827e-c174afedb722"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a575458-a6da-4e54-8688-83e155f2de06",
      "metadata": {
        "id": "5a575458-a6da-4e54-8688-83e155f2de06"
      },
      "source": [
        "- Se aplicarmos uma taxa de dropout de 0,5 (50%), os valores não descartados serão escalados proporcionalmente por um fator de 1/0,5 = 2.  \n",
        "- O escalonamento é calculado pela fórmula 1 / (1 - `dropout_rate`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de578db-8289-41d6-b377-ef645751e33f",
      "metadata": {
        "id": "0de578db-8289-41d6-b377-ef645751e33f",
        "outputId": "a44b3590-f122-482b-e13a-f8219d33ef2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "example = torch.ones(6, 6) # create a matrix of ones\n",
        "\n",
        "print(dropout(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
      "metadata": {
        "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
        "outputId": "c360a3e7-e112-4d5a-8c35-37e694f07dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269df5c8-3e25-49d0-95d3-bb232287404f",
      "metadata": {
        "id": "269df5c8-3e25-49d0-95d3-bb232287404f"
      },
      "source": [
        "- Observe que as saídas resultantes do dropout podem parecer diferentes dependendo do seu sistema operacional; você pode ler mais sobre essa inconsistência [aqui no rastreador de problemas do PyTorch](https://github.com/pytorch/pytorch/issues/121595)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7",
      "metadata": {
        "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7"
      },
      "source": [
        "### 3.5.3 Implementando uma classe compacta de autoatenção causal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c41d29-1933-43dc-ada6-2dbb56287204",
      "metadata": {
        "id": "09c41d29-1933-43dc-ada6-2dbb56287204"
      },
      "source": [
        "- Agora, estamos prontos para implementar uma versão funcional de autoatenção, incluindo as máscaras de causalidade e dropout.  \n",
        "- Uma coisa a mais é implementar o código para lidar com lotes (batches) consistindo em mais de uma entrada, de modo que nossa classe `CausalAttention` suporte as saídas em lote produzidas pelo data loader que implementamos no capítulo 2.  \n",
        "- Para simplificar, para simular tal entrada em lote, duplicamos o exemplo de texto de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
      "metadata": {
        "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
        "outputId": "58f30e11-2425-4347-953c-b685d5b867de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
      "metadata": {
        "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
        "outputId": "ff587af3-445f-49d3-f343-eca5dec1f094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # Nova dimensão de lote b\n",
        "        # Para entradas onde `num_tokens` excede `context_length`, isso resultará em erros\n",
        "        # na criação da máscara abaixo.\n",
        "        # Na prática, isso não é um problema, pois o LLM (capítulos 4-7) garante que as entradas\n",
        "        # não excedam `context_length` antes de chegar a este método forward.\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` para levar em conta os casos em que o número de tokens no lote é menor do que o tamanho de contexto suportado\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4333d12-17e4-4bb5-9d83-54b3a32618cd",
      "metadata": {
        "id": "c4333d12-17e4-4bb5-9d83-54b3a32618cd"
      },
      "source": [
        "- Observe que o dropout é aplicado apenas durante o treinamento, não durante a inferência."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a554cf47-558c-4f45-84cd-bf9b839a8d50",
      "metadata": {
        "id": "a554cf47-558c-4f45-84cd-bf9b839a8d50"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/23.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c",
      "metadata": {
        "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c"
      },
      "source": [
        "## 3.6 Estendendo a atenção de cabeça única para atenção multi-cabeça"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9",
      "metadata": {
        "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9"
      },
      "source": [
        "### 3.6.1 Empilhando várias camadas de atenção de cabeça única"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70766faf-cd53-41d9-8a17-f1b229756a5a",
      "metadata": {
        "id": "70766faf-cd53-41d9-8a17-f1b229756a5a"
      },
      "source": [
        "- Abaixo está um resumo da autoatenção implementada anteriormente (máscaras de causalidade e dropout não mostradas por simplicidade).  \n",
        "\n",
        "- Isso também é chamado de atenção de cabeça única:\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/24.webp\" width=\"400px\">\n",
        "\n",
        "- Simplesmente empilhamos múltiplos módulos de atenção de cabeça única para obter um módulo de atenção multi-cabeça:\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/25.webp\" width=\"400px\">\n",
        "\n",
        "- A ideia principal por trás da atenção multi-cabeça é executar o mecanismo de atenção várias vezes (em paralelo) com diferentes projeções lineares aprendidas. Isso permite que o modelo preste atenção conjunta em informações de diferentes subespaços de representação em diferentes posições."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
      "metadata": {
        "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
        "outputId": "14e70560-a5af-4e24-ce4e-9744f4177719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1] # This is the number of tokens\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in, d_out, context_length, 0.0, num_heads=2\n",
        ")\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193d3d2b-2578-40ba-b791-ea2d49328e48",
      "metadata": {
        "id": "193d3d2b-2578-40ba-b791-ea2d49328e48"
      },
      "source": [
        "- Na implementação acima, a dimensão do embedding é 4, porque definimos `d_out=2` como a dimensão do embedding para os vetores de chave, consulta e valor, assim como para o vetor de contexto. E como temos 2 cabeças de atenção, a dimensão do embedding de saída é $2*2=4$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e",
      "metadata": {
        "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e"
      },
      "source": [
        "### 3.6.2 Implementando atenção multi-cabeça com divisões de peso"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7",
      "metadata": {
        "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7"
      },
      "source": [
        "- Embora a implementação acima seja uma implementação intuitiva e totalmente funcional da atenção multi-cabeça (envolvendo a implementação de atenção de cabeça única `CausalAttention` mencionada anteriormente), podemos escrever uma classe independente chamada `MultiHeadAttention` para alcançar o mesmo resultado.\n",
        "\n",
        "- Não concatenamos as cabeças de atenção únicas para esta classe independente `MultiHeadAttention`.  \n",
        "- Em vez disso, criamos matrizes de pesos W_query, W_key e W_value únicas e, em seguida, dividimos essas matrizes em matrizes individuais para cada cabeça de atenção."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
      "metadata": {
        "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
        "outputId": "8183fd50-7991-4214-e5fb-700447b58643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduzir a dimensão da projeção para coincidir com a dimensão de saída desejada\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        # Como em CausalAttention, para entradas onde num_tokens excede context_length,\n",
        "        # isso resultará em erros na criação da máscara mais abaixo.\n",
        "        # Na prática, isso não é um problema, pois o LLM (capítulos 4-7) garante que as entradas\n",
        "        # não excedam `context_length` antes de chegar a este método forward.\n",
        "\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # Dividimos implicitamente a matriz adicionando uma dimensão num_heads\n",
        "        # Desenrolar a última dimensão: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Calcular a atenção por produto escalar escalado (também conhecida como autoatenção) com uma máscara causal\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Máscara original truncada para o número de tokens e convertida para booleano\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use a máscara para preencher os scores de atenção\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine as cabeças, onde self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb",
      "metadata": {
        "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb"
      },
      "source": [
        "- Observe que o acima é essencialmente uma versão reescrita de `MultiHeadAttentionWrapper`, que é mais eficiente.  \n",
        "- A saída resultante parece um pouco diferente, já que as inicializações de peso aleatórias diferem, mas ambas são implementações totalmente funcionais que podem ser usadas na classe GPT que implementaremos nos próximos capítulos.  \n",
        "- Além disso, note que adicionamos uma camada de projeção linear (`self.out_proj`) à classe `MultiHeadAttention` acima. Isso é simplesmente uma transformação linear que não altera as dimensões. É uma convenção padrão usar tal camada de projeção na implementação de LLM, mas não é estritamente necessário (pesquisas recentes mostraram que ela pode ser removida sem afetar o desempenho do modelo; veja a seção de leituras adicionais no final deste capítulo)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe5d396-c990-45dc-9908-2c621461f851",
      "metadata": {
        "id": "dbe5d396-c990-45dc-9908-2c621461f851"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65",
      "metadata": {
        "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65"
      },
      "source": [
        "- Note that if you are interested in a compact and efficient implementation of the above, you can also consider the [`torch.nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) class in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "363701ad-2022-46c8-9972-390d2a2b9911",
      "metadata": {
        "id": "363701ad-2022-46c8-9972-390d2a2b9911"
      },
      "source": [
        "- Since the above implementation may look a bit complex at first glance, let's look at what happens when executing `attn_scores = queries @ keys.transpose(2, 3)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8cfc1ae-78ab-4faa-bc73-98bd054806c9",
      "metadata": {
        "id": "e8cfc1ae-78ab-4faa-bc73-98bd054806c9",
        "outputId": "8b527f78-4586-4ffc-f863-3aa43bb3ddb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ],
      "source": [
        "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
        "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
        "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "\n",
        "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
        "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
        "\n",
        "print(a @ a.transpose(2, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0587b946-c8f2-4888-adbf-5a5032fbfd7b",
      "metadata": {
        "id": "0587b946-c8f2-4888-adbf-5a5032fbfd7b"
      },
      "source": [
        "- In this case, the matrix multiplication implementation in PyTorch will handle the 4-dimensional input tensor so that the matrix multiplication is carried out between the 2 last dimensions (num_tokens, head_dim) and then repeated for the individual heads\n",
        "\n",
        "- For instance, the following becomes a more compact way to compute the matrix multiplication for each head separately:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053760f1-1a02-42f0-b3bf-3d939e407039",
      "metadata": {
        "id": "053760f1-1a02-42f0-b3bf-3d939e407039",
        "outputId": "6a004e65-13fb-4994-b08f-402d6aa0b051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First head:\n",
            " tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n",
            "\n",
            "Second head:\n",
            " tensor([[0.4391, 0.7003, 0.5903],\n",
            "        [0.7003, 1.3737, 1.0620],\n",
            "        [0.5903, 1.0620, 0.9912]])\n"
          ]
        }
      ],
      "source": [
        "first_head = a[0, 0, :, :]\n",
        "first_res = first_head @ first_head.T\n",
        "print(\"First head:\\n\", first_res)\n",
        "\n",
        "second_head = a[0, 1, :, :]\n",
        "second_res = second_head @ second_head.T\n",
        "print(\"\\nSecond head:\\n\", second_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec671bf-7938-4304-ad1e-75d9920e7f43",
      "metadata": {
        "id": "dec671bf-7938-4304-ad1e-75d9920e7f43"
      },
      "source": [
        "# Summary and takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa3e4113-ffca-432c-b3ec-7a50bd15da25",
      "metadata": {
        "id": "fa3e4113-ffca-432c-b3ec-7a50bd15da25"
      },
      "source": [
        "- See the [./multihead-attention.ipynb](./multihead-attention.ipynb) code notebook, which is a concise version of the data loader (chapter 2) plus the multi-head attention class that we implemented in this chapter and will need for training the GPT model in upcoming chapters\n",
        "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}