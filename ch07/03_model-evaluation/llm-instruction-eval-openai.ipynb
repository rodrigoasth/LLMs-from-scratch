{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
      "metadata": {
        "id": "136a4efe-fb99-4311-8679-e0a5b6282755"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
      "metadata": {
        "tags": [],
        "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4"
      },
      "source": [
        "# Avaliação de Respostas de Instrução Usando a API do OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a128651b-f326-4232-a994-42f38b7ed520",
      "metadata": {
        "id": "a128651b-f326-4232-a994-42f38b7ed520"
      },
      "source": [
        "- Este notebook utiliza a API GPT-4 da OpenAI para avaliar respostas de LLMs ajustados por instrução com base em um dataset no formato JSON que inclui as respostas geradas pelo modelo, por exemplo:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"instruction\": \"What is the atomic number of helium?\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"The atomic number of helium is 2.\",               # <-- The target given in the test set\n",
        "    \"model 1 response\": \"\\nThe atomic number of helium is 2.0.\", # <-- Response by an LLM\n",
        "    \"model 2 response\": \"\\nThe atomic number of helium is 3.\"    # <-- Response by a 2nd LLM\n",
        "},\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267ba0d1-b884-42df-85bd-0be746fd47a5",
      "metadata": {
        "id": "267ba0d1-b884-42df-85bd-0be746fd47a5"
      },
      "outputs": [],
      "source": [
        "# pip install -r requirements-extra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "63610acc-db94-437f-8d38-e99dca0299cb",
      "metadata": {
        "id": "63610acc-db94-437f-8d38-e99dca0299cb",
        "outputId": "1ed3df9f-82eb-4136-cdd7-434ff3d79cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openai version: 1.61.1\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"openai\",  # OpenAI API\n",
        "        \"tqdm\",    # Progress bar\n",
        "        ]\n",
        "\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
      "metadata": {
        "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5"
      },
      "source": [
        "## Teste da API OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
      "metadata": {
        "id": "9558a522-650d-401a-84fc-9fd7b1f39da7"
      },
      "source": [
        "- Primeiro, vamos testar se a API da OpenAI está configurada corretamente.\n",
        "- Se você ainda não tem uma conta, precisa criar uma em https://platform.openai.com/\n",
        "- Observe que você também precisará adicionar saldo à sua conta, pois a API do GPT-4 não é gratuita (veja (see https://platform.openai.com/settings/organization/billing/overview)\n",
        "- Executar os experimentos e criar aproximadamente 200 avaliações usando o código neste notebook custa cerca de $0,26 (26 centavos de dólar) no momento desta escrita."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
      "metadata": {
        "id": "89343a84-0ddc-42fc-bf50-298a342b93c0"
      },
      "source": [
        "- Primeiro, precisamos fornecer nossa chave secreta da API da OpenAI, que pode ser encontrada em https://platform.openai.com/api-keys\n",
        "- Certifique-se de não compartilhar essa chave com ninguém!\n",
        "- Adicione essa chave secreta (\"sk-...\") ao arquivo config.json nesta pasta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
      "metadata": {
        "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load API key from a JSON file.\n",
        "# Make sure to replace \"sk-...\" with your actual API key from https://platform.openai.com/api-keys\n",
        "#with open(\"config.json\", \"r\") as config_file:\n",
        "#    config = json.load(config_file)\n",
        "#    api_key = config[\"OPENAI_API_KEY\"]\n",
        "\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
      "metadata": {
        "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876"
      },
      "source": [
        "- First, let's try the API with a simple example to make sure it works as intended:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "08e9ef2e-e816-4283-840e-43625791ad33",
      "metadata": {
        "id": "08e9ef2e-e816-4283-840e-43625791ad33",
        "outputId": "5ac8651e-6844-4325-acf0-78456247dd4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "def run_chatgpt(prompt, client, model=\"gpt-4o-mini\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.0,\n",
        "        seed=123,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"Respond with 'hello world' if you got this message.\"\n",
        "run_chatgpt(prompt, client)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LrdgFanBJeKC",
        "outputId": "28bfafed-6679-4581-97cb-ccf138f5bfad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LrdgFanBJeKC",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
      "metadata": {
        "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d"
      },
      "source": [
        "## Carregar entradas JSON."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
      "metadata": {
        "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a"
      },
      "source": [
        "- Here, we assume that we saved the test dataset and the model responses as a JSON file that we can load as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
      "metadata": {
        "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
        "outputId": "e038069e-8925-4a2b-f6db-0b86bd9fef12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 110\n"
          ]
        }
      ],
      "source": [
        "json_file = \"instruction-data-with-response.json\"\n",
        "\n",
        "with open(json_file, \"r\") as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "print(\"Number of entries:\", len(json_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
      "metadata": {
        "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66"
      },
      "source": [
        "- A estrutura deste arquivo é a seguinte, onde temos a resposta fornecida no conjunto de testes (`'output'`) e as respostas de dois modelos diferentes (`'model 1 response'` e `'model 2 response'`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
      "metadata": {
        "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
        "outputId": "64070e27-abe5-454f-c6cc-e12ee6177002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Rewrite the sentence using a simile.',\n",
              " 'input': 'The car is very fast.',\n",
              " 'output': 'The car is as fast as lightning.',\n",
              " 'model_response': 'The car is as fast as a bullet.'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "json_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
      "metadata": {
        "id": "fcf0331b-6024-4bba-89a9-a088b14a1046"
      },
      "source": [
        "- Below is a small utility function that formats the input for visualization purposes later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
      "metadata": {
        "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. Write a response that \"\n",
        "        f\"appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "    instruction_text + input_text\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a55283-7d51-4136-ba60-f799d49f4098",
      "metadata": {
        "id": "39a55283-7d51-4136-ba60-f799d49f4098"
      },
      "source": [
        "- Now, let's try the OpenAI API to compare the model responses (we only evaluate the first 5 responses for a visual comparison):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "735cc089-d127-480a-b39d-0782581f0c41",
      "metadata": {
        "collapsed": true,
        "id": "735cc089-d127-480a-b39d-0782581f0c41",
        "outputId": "6f3017d4-939d-42e3-951a-a71b3c120769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a bullet.\n",
            "\n",
            "Score:\n",
            ">> To score the model response \"The car is as fast as a bullet.\" based on the instruction to rewrite the sentence using a simile, we need to consider the effectiveness and appropriateness of the simile used.\n",
            "\n",
            "1. **Accuracy of Simile**: The original sentence \"The car is very fast.\" is effectively transformed into a simile by comparing the car's speed to something universally recognized as fast. Both \"lightning\" and \"a bullet\" are commonly used in similes to denote high speed. Thus, the model's choice of \"a bullet\" is accurate and maintains the meaning of the original sentence.\n",
            "\n",
            "2. **Relevance and Common Usage**: The simile \"as fast as a bullet\" is a standard and widely understood comparison that conveys extreme speed effectively. It is comparable in effectiveness to \"as fast as lightning,\" which was the given correct output.\n",
            "\n",
            "3. **Creativity and Clarity**: The simile used by the model is clear and immediately conveys the intended meaning without ambiguity. It is also a creative way to express speed, though it is a common expression.\n",
            "\n",
            "4. **Fulfillment of Task Requirements**: The task was to rewrite the sentence using a simile. The model response successfully fulfills this requirement by using a well-known simile that fits the context of describing a car's speed.\n",
            "\n",
            "Given these considerations, the model response \"The car is as fast as a bullet.\" scores highly as it effectively uses a simile to convey the original meaning, employs a commonly understood comparison, and fulfills the task requirements accurately. \n",
            "\n",
            "**Score: 95/100**\n",
            "\n",
            "The slight deduction might only be due to the fact that there could be other similes that might convey an even more vivid image or a more creative comparison, but this is minor since the response is highly effective as provided.\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
            "\n",
            "Score:\n",
            ">> The model response \"The type of cloud associated with thunderstorms is a cumulus cloud.\" is incorrect. The correct type of cloud associated with thunderstorms is cumulonimbus, not cumulus. Cumulus clouds are generally associated with fair weather when they are in their basic form, though they can develop into cumulonimbus clouds under the right conditions.\n",
            "\n",
            "Given the incorrect information in the response, the score should reflect the accuracy of the content in relation to the question asked. Since the response provides an incorrect type of cloud, it fails to meet the requirement of the instruction accurately.\n",
            "\n",
            "Score: 20/100\n",
            "\n",
            "This score reflects that while the response is related to the topic of clouds and does mention a type of cloud, it inaccurately identifies the type associated with thunderstorms, which is a critical error in the context of the question.\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "\n",
            "Score:\n",
            ">> The model response \"The author of 'Pride and Prejudice' is Jane Austen.\" accurately identifies Jane Austen as the author of 'Pride and Prejudice'. The response is correct, clear, and directly addresses the instruction provided. Although the response format slightly elaborates beyond a simple naming by including a full sentence structure, it remains highly relevant and informative.\n",
            "\n",
            "Given the correctness and clarity, the response should be scored highly. The only consideration for not awarding a perfect score might be if the instruction explicitly demanded the shortest possible answer format, which in this case, it did not. Therefore, the response effectively fulfills the task requirements.\n",
            "\n",
            "Score: 98/100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The periodic symbol for chlorine is Cl.\n",
            "\n",
            "Model response:\n",
            ">> The periodic symbol for chlorine is C.\n",
            "\n",
            "Score:\n",
            ">> The model response \"The periodic symbol for chlorine is C.\" is incorrect. The correct periodic symbol for chlorine is \"Cl,\" not \"C.\" The symbol \"C\" actually represents carbon, not chlorine. Therefore, the response provides incorrect information that could lead to confusion or misunderstanding in a scientific context.\n",
            "\n",
            "To score the response:\n",
            "- Accuracy is the most critical factor here, as the question specifically asks for a factual piece of information.\n",
            "- The response is confidently presented but factually incorrect.\n",
            "\n",
            "Given these considerations, the response would score very low on accuracy. However, it does correctly identify that a periodic symbol is being asked for and provides a symbol, albeit the wrong one.\n",
            "\n",
            "Score: 10/100\n",
            "\n",
            "This score reflects that the response addresses the format of the question (providing a periodic symbol) but fails in providing the correct information.\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The corrected sentence should be: 'It's time to go home.'\n",
            "\n",
            "Model response:\n",
            ">> It's time to go home.\n",
            "\n",
            "Score:\n",
            ">> The model response to the instruction is \"It's time to go home.\" This response correctly addresses the task of correcting the punctuation in the given sentence. The original sentence \"Its time to go home.\" lacked the necessary apostrophe in \"It's\" to correctly form the contraction for \"It is.\"\n",
            "\n",
            "The model response correctly inserts the apostrophe, resulting in \"It's time to go home.\" This correction is accurate and directly fixes the punctuation error in the input sentence. However, the response does not include the framing requested in the correct output example, which specifies that the response should be formatted as: \"The corrected sentence should be: 'It's time to go home.'\"\n",
            "\n",
            "Despite this, the core task of correcting the punctuation was accomplished perfectly. Therefore, the response should be scored highly for accuracy in punctuation correction but slightly lower for not following the exact output format specified.\n",
            "\n",
            "**Score: 90/100** \n",
            "\n",
            "This score reflects the high accuracy in punctuation correction while slightly penalizing for not adhering to the exact output format provided in the example.\n",
            "\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "for entry in json_data[:5]:\n",
        "    prompt = (f\"Given the input `{format_input(entry)}` \"\n",
        "              f\"and correct output `{entry['output']}`, \"\n",
        "              f\"score the model response `{entry['model_response']}`\"\n",
        "              f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "              )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", run_chatgpt(prompt, client))\n",
        "    print(\"\\n-------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
      "metadata": {
        "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a"
      },
      "source": [
        "- Note that the responses are very verbose; to quantify which model is better, we only want to return the scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468",
      "metadata": {
        "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate_model_scores(json_data, json_key, client):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the number only.\"\n",
        "        )\n",
        "        score = run_chatgpt(prompt, client)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71974dea-31ed-49af-abba-5c858bbbf49c",
      "metadata": {
        "id": "71974dea-31ed-49af-abba-5c858bbbf49c"
      },
      "source": [
        "- Please note that the response scores may vary because OpenAI's GPT models are not deterministic despite setting a random number seed, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b071ce84-1866-427f-a272-b46700f364b2",
      "metadata": {
        "id": "b071ce84-1866-427f-a272-b46700f364b2"
      },
      "source": [
        "- Let's now apply this evaluation to the whole dataset and compute the average score of each model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
      "metadata": {
        "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
        "outputId": "551afede-7241-4664-b988-6cf917bc6a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring entries: 100%|██████████| 110/110 [00:54<00:00,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model_response\n",
            "Number of scores: 110 of 110\n",
            "Average score: 41.32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'scores/gpt4-model_response.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-222607461a0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Optionally save the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"gpt4-{model.replace(' ', '-')}.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scores/gpt4-model_response.json'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "#for model in (\"model_response\", \"model 2 response\"):\n",
        "model = \"model_response\"\n",
        "\n",
        "scores = generate_model_scores(json_data, model, client)\n",
        "print(f\"\\n{model}\")\n",
        "print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
        "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
        "\n",
        "# Optionally save the scores\n",
        "save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n",
        "with open(save_path, \"w\") as file:\n",
        "    json.dump(scores, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8169d534-1fec-43c4-9550-5cb701ff7f05",
      "metadata": {
        "id": "8169d534-1fec-43c4-9550-5cb701ff7f05"
      },
      "source": [
        "- Based on the evaluation above, we can say that the 1st model is substantially better than the 2nd model"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}